<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=description content="An example site for the clean and configurable Hugo Classless theme."><link href=/2026/01/sllm_v040/ rel=alternate type=application/rss+xml title=mozanunal.com><link id=theme-style rel=stylesheet href=/css/classless.20c8e211f6768ba295effc3e101a31bfe248db7295bb122355bdf893c6cee731.css integrity="sha256-IMjiEfZ2i6KV7/w+EBoxv+JI23KVuxIjVb34k8bO5zE="><style>pre code,code{-webkit-text-size-adjust:100%;text-size-adjust:100%}img{display:block;margin-left:auto;margin-right:auto}figcaption{display:block;margin-left:auto;margin-right:auto}nav{display:flex;justify-content:space-between;align-items:center;flex-wrap:wrap;gap:1rem;overflow:visible;font-size:1.4rem}nav ul{display:flex;list-style:none;padding:0;margin:0;gap:.5rem}</style><link rel=stylesheet href=/css/syntax-light.min.d1e9974bf1fe0f3bfae6a7af3a04c1e749276b8efbbf6d3397998585f82443f0.css media="(prefers-color-scheme: light)"><link rel=stylesheet href=/css/syntax-dark.min.22964c63865be9217cedda7ac3dbd0143f8f41b3a0f681ef71e845b18a33a4f3.css media="(prefers-color-scheme: dark)"><title>Sllm.nvim v0.4.0: Modes as YAML, Radical Simplicity | mozanunal.com</title><link rel=icon href=/favicon.svg type=image/svg+xml><link rel="shortcut icon" href=/favicon.ico><link rel=apple-touch-icon href=/apple-touch-icon.png></head><body><header><nav><ul><li><strong>mozanunal.com</strong></li></ul><ul><li><a href=/>Home</a></li><li><a href=/posts/>Posts</a></li><li><a href=/projects/>Projects</a></li><li><a href=/papers/>Papers</a></li></ul></nav></header><main><article><h1>Sllm.nvim v0.4.0: Modes as YAML, Radical Simplicity</h1><p><em>January 23, 2026</em></p><div><p>I&rsquo;m excited to announce
<strong><a href=https://github.com/mozanunal/sllm.nvim>sllm.nvim v0.4.0</a></strong> – a significant
release that doubles down on the plugin&rsquo;s core philosophy: modes are just YAML,
tools are just Python functions, and you stay in control.</p><h2 id=the-philosophy-recap>The Philosophy Recap</h2><p>For those new to <code>sllm.nvim</code>, it&rsquo;s a thin Neovim wrapper around Simon Willison&rsquo;s
<a href=https://llm.datasette.io/><code>llm</code></a> CLI. The design philosophy is simple:</p><ul><li><strong>You control context</strong> – Add files, selections, URLs, diagnostics, or shell
output explicitly. The LLM sees exactly what you give it.</li><li><strong>Radical simplicity</strong> – The plugin delegates all heavy lifting to the
battle-tested <code>llm</code> CLI (~500 lines of Lua).</li><li><strong>CLI ecosystem access</strong> – 100+ models via OpenRouter, local models via
Ollama, plus plugins for PDFs, web search, and more.</li></ul><h2 id=whats-new-in-v040>What&rsquo;s New in v0.4.0</h2><h3 id=modes-are-just-yaml>Modes Are Just YAML</h3><p>The template system has been refined and expanded. Creating a custom mode is now
as simple as writing a 5-line YAML file:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=c># ~/.config/io.datasette.llm/templates/my_reviewer.yaml</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=nt>system</span><span class=p>:</span><span class=w> </span><span class=p>|</span><span class=sd>
</span></span></span><span class=line><span class=cl><span class=sd>  You are a code reviewer. Be concise and actionable.
</span></span></span><span class=line><span class=cl><span class=sd>  Focus on bugs, not style.</span><span class=w>
</span></span></span></code></pre></div><p>The plugin ships with four default modes:</p><ul><li><strong><code>sllm_chat</code></strong> – General-purpose conversations</li><li><strong><code>sllm_read</code></strong> – Explore codebases with read-only tools (list, read, grep,
glob)</li><li><strong><code>sllm_agent</code></strong> – Full agentic mode with bash/read/write/edit/grep tools</li><li><strong><code>sllm_complete</code></strong> – Inline completion at cursor position</li></ul><p>Switch modes with <code>&lt;leader>sM</code> or <code>/template</code>. Edit them with
<code>llm templates edit sllm_agent</code>.</p><h3 id=on-the-fly-tools-refined>On-the-fly Tools Refined</h3><p>The on-the-fly function registration (<code>/add-function</code> or <code>&lt;leader>sx</code> →
&ldquo;add-function&rdquo;) continues to be a standout feature. Select any Python function
in your buffer and the LLM can use it immediately:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>fetch_user_data</span><span class=p>(</span><span class=n>user_id</span><span class=p>:</span> <span class=nb>str</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>str</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Fetch user data from the database.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Args:
</span></span></span><span class=line><span class=cl><span class=s2>        user_id: The user&#39;s ID
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># Your implementation here</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>json</span><span class=o>.</span><span class=n>dumps</span><span class=p>(</span><span class=n>db</span><span class=o>.</span><span class=n>get_user</span><span class=p>(</span><span class=n>user_id</span><span class=p>))</span>
</span></span></code></pre></div><p>Select it, run <code>/add-function</code>, and the LLM can now query your database
mid-conversation. No configuration, no restart needed.</p><h3 id=slash-commands-system>Slash Commands System</h3><p>A new unified command system replaces scattered keymaps with discoverable
actions:</p><ul><li><code>&lt;leader>sx</code> opens a fuzzy command picker</li><li>Type <code>/command</code> directly in the prompt (e.g., <code>/new</code>, <code>/model</code>, <code>/add-file</code>)</li></ul><p>Commands are organized by purpose:</p><ul><li><strong>Chat</strong>: <code>/new</code>, <code>/history</code>, <code>/cancel</code></li><li><strong>Context</strong>: <code>/add-file</code>, <code>/add-url</code>, <code>/add-selection</code>, <code>/add-diagnostics</code>,
<code>/add-output</code>, <code>/add-tool</code>, <code>/add-function</code>, <code>/clear-context</code></li><li><strong>Model</strong>: <code>/model</code>, <code>/template</code>, <code>/online</code>, <code>/system</code></li><li><strong>Options</strong>: <code>/options</code>, <code>/set-option</code>, <code>/reset-options</code></li><li><strong>Copy</strong>: <code>/copy-code</code>, <code>/copy-code-first</code>, <code>/copy-response</code></li></ul><h3 id=inline-completion>Inline Completion</h3><p><code>&lt;leader>&lt;Tab></code> triggers inline completion at your cursor position. The
<code>sllm_complete</code> template sends buffer context and returns raw code suitable for
direct insertion – no markdown wrapping, just completions that fit.</p><h3 id=token-tracking-in-winbar>Token Tracking in Winbar</h3><p>The LLM buffer&rsquo;s winbar now displays token usage stats and estimated costs,
giving you visibility into your API consumption at a glance.</p><h3 id=prepost-hooks>Pre/Post Hooks</h3><p>Automate context gathering with hooks:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-lua data-lang=lua><span class=line><span class=cl><span class=n>require</span><span class=p>(</span><span class=s1>&#39;sllm&#39;</span><span class=p>).</span><span class=n>setup</span><span class=p>({</span>
</span></span><span class=line><span class=cl>  <span class=n>pre_hooks</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span> <span class=n>command</span> <span class=o>=</span> <span class=s1>&#39;git diff --cached&#39;</span><span class=p>,</span> <span class=n>add_to_context</span> <span class=o>=</span> <span class=kc>true</span> <span class=p>},</span>
</span></span><span class=line><span class=cl>  <span class=p>},</span>
</span></span><span class=line><span class=cl>  <span class=n>post_hooks</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span> <span class=n>command</span> <span class=o>=</span> <span class=s1>&#39;notify-send &#34;sllm.nvim&#34; &#34;Response ready&#34;&#39;</span> <span class=p>},</span>
</span></span><span class=line><span class=cl>  <span class=p>},</span>
</span></span><span class=line><span class=cl><span class=p>})</span>
</span></span></code></pre></div><p>Now every prompt automatically includes your staged changes.</p><h2 id=quick-start>Quick Start</h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Install llm and a provider</span>
</span></span><span class=line><span class=cl>brew install llm <span class=o>&amp;&amp;</span> llm install llm-openrouter <span class=o>&amp;&amp;</span> llm keys <span class=nb>set</span> openrouter
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-lua data-lang=lua><span class=line><span class=cl><span class=c1>-- lazy.nvim</span>
</span></span><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=s1>&#39;mozanunal/sllm.nvim&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>config</span> <span class=o>=</span> <span class=kr>function</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>require</span><span class=p>(</span><span class=s1>&#39;sllm&#39;</span><span class=p>).</span><span class=n>setup</span><span class=p>()</span>
</span></span><span class=line><span class=cl>  <span class=kr>end</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>Press <code>&lt;leader>ss</code> to start chatting.</p><h2 id=documentation>Documentation</h2><ul><li><a href=https://github.com/mozanunal/sllm.nvim/blob/main/doc/modes.md>Modes & Templates</a></li><li><a href=https://github.com/mozanunal/sllm.nvim/blob/main/doc/slash_commands.md>Slash Commands</a></li><li><a href=https://github.com/mozanunal/sllm.nvim/blob/main/doc/configure.md>Configuration</a></li><li><a href=https://github.com/mozanunal/sllm.nvim/blob/main/doc/hooks.md>Hooks</a></li></ul><h2 id=try-it-out>Try It Out</h2><p>If this CLI-first, co-pilot-centric approach resonates with you, give
<code>sllm.nvim</code> a try:
<strong><a href=https://github.com/mozanunal/sllm.nvim>mozanunal/sllm.nvim</a></strong></p><p>Feedback, issues, and contributions are always welcome!</p></div></article></main><footer><p>&copy; 2026 M.Ozan Unal.</p></footer></body><link rel=stylesheet href=/libs/katex/katex.min.v0.16.9.css><script defer src=/libs/katex/katex.min.v0.16.9.js></script><script defer src=/libs/katex/auto-render.v0.16.9.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1},{left:"$",right:"$",display:!1}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-007KSW65JL"></script><script>var doNotTrack=!1,dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes";if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-007KSW65JL")}</script></html>