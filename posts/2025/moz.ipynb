{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0c852df-a6f2-4e35-aa7d-e644d87ddab2",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " # ðŸš€ Executable Guide: Mastering Advanced Parquet in Python\n",
    "\n",
    " Apache Parquet is the undisputed king of columnar storage for analytics. If you're using Python for data work, you've likely used `pandas.to_parquet()` and `pandas.read_parquet()`. But beneath this simple interface lies a powerful engine with features that can drastically improve your storage costs and query speeds.\n",
    "\n",
    " This post is an **executable guide**. You can run the code snippets in order to follow along and see the results for yourself. We'll dive into advanced features that every data professional should know."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995f4adf-35a3-4889-b485-ae88ae1ced6f",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ### Setup\n",
    "\n",
    " First, let's install the necessary libraries. We'll use `pyarrow`, the standard for working with Parquet in Python, along with `pandas` and `numpy`.\n",
    "\n",
    " `pip install pandas pyarrow numpy`\n",
    "\n",
    " Now, let's import them and prepare a directory for our files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfff7b79-fcf6-4529-b542-cef0268f2743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyArrow version: 20.0.0\n",
      "Data will be stored in: parquet_data\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Create a directory to store our parquet files\n",
    "DATA_DIR = \"parquet_data\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"PyArrow version: {pa.__version__}\")\n",
    "print(f\"Data will be stored in: {os.path.relpath(DATA_DIR)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021da885-e3bc-4e4f-8629-2c0ac698f797",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ### 1. Compression Deep Dive: Beyond the Defaults\n",
    "\n",
    " Parquet is always compressed, but you have a choice of codecs. The default is `snappy`, which is fast but not the most space-efficient. Let's compare it with `gzip` (better compression, slower) and `zstd` (often the best of both worlds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85b90e8-e6f9-48d6-b6d4-5b2e6b0c4e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing files with different compressions...\n",
      "\n",
      "--- File Sizes ---\n",
      "data.gzip.parquet        : 9.58 MB\n",
      "data.snappy.parquet      : 12.34 MB\n",
      "data.uncompressed.parquet: 16.15 MB\n",
      "data.zstd.parquet        : 9.12 MB\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Create a sample DataFrame\n",
    "num_rows = 1_000_000\n",
    "data = {\n",
    "    'id': np.arange(num_rows),\n",
    "    'category': np.random.choice(['A', 'B', 'C', 'D', 'E'], num_rows, p=[0.1, 0.2, 0.3, 0.2, 0.2]),\n",
    "    'value': np.random.randn(num_rows)\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save with different compression codecs\n",
    "print(\"Writing files with different compressions...\")\n",
    "df.to_parquet(f'{DATA_DIR}/data.snappy.parquet', compression='snappy')\n",
    "df.to_parquet(f'{DATA_DIR}/data.gzip.parquet', compression='gzip')\n",
    "df.to_parquet(f'{DATA_DIR}/data.zstd.parquet', compression='zstd')\n",
    "df.to_parquet(f'{DATA_DIR}/data.uncompressed.parquet', compression='None')\n",
    "\n",
    "# Check file sizes\n",
    "print(\"\\n--- File Sizes ---\")\n",
    "for filename in sorted(os.listdir(DATA_DIR)):\n",
    "    if filename.endswith('.parquet') and filename.startswith('data.'):\n",
    "        size_mb = os.path.getsize(f'{DATA_DIR}/{filename}') / (1024 * 1024)\n",
    "        print(f'{filename:<25}: {size_mb:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58932833-05f4-42a1-9508-490fab69e82d",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " **Explanation:** You'll notice `zstd` and `gzip` create significantly smaller files than `snappy` or no compression. For archival data or network-bound systems, choosing a high-ratio codec like `zstd` can lead to major cost savings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecc0a0b-1382-47d3-9041-aef16ae6c327",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ### 2. Lightning-Fast Queries with Predicate Pushdown\n",
    "\n",
    " This is Parquet's superpower. Instead of reading an entire 100 GB file to find a few rows, you can push a filter *down* to the storage layer, which then only reads the relevant chunks of data (**row groups**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990cd753-fb12-4819-b4ca-c942110835c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Method 1: Read all data, then filter ---\n",
      "Filtered 100458 rows in 0.0794 seconds.\n",
      "\n",
      "--- Method 2: Predicate Pushdown with PyArrow ---\n",
      "Filtered 100458 rows in 0.0110 seconds.\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Write the file with a defined row group size for demonstration\n",
    "pq.write_table(\n",
    "    pa.Table.from_pandas(df),\n",
    "    f'{DATA_DIR}/filtered_data.parquet',\n",
    "    row_group_size=100_000 # Each group has 100k rows\n",
    ")\n",
    "\n",
    "# --- Method 1: The slow way (Read all, then filter in pandas) ---\n",
    "print(\"\\n--- Method 1: Read all data, then filter ---\")\n",
    "start_time = time.time()\n",
    "df_full = pd.read_parquet(f'{DATA_DIR}/filtered_data.parquet')\n",
    "df_filtered = df_full[df_full['category'] == 'A']\n",
    "duration = time.time() - start_time\n",
    "print(f\"Filtered {len(df_filtered)} rows in {duration:.4f} seconds.\")\n",
    "\n",
    "# --- Method 2: The fast way (Predicate Pushdown) ---\n",
    "# The filter format is a list of tuples: [(<col>, <op>, <val>),...]\n",
    "filters = [('category', '==', 'A')]\n",
    "print(\"\\n--- Method 2: Predicate Pushdown with PyArrow ---\")\n",
    "start_time = time.time()\n",
    "df_pushed = pq.read_table(f'{DATA_DIR}/filtered_data.parquet', filters=filters).to_pandas()\n",
    "duration = time.time() - start_time\n",
    "print(f\"Filtered {len(df_pushed)} rows in {duration:.4f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5c6aea-56df-4a21-a09a-02480fc63e23",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " **Explanation:** The second method is dramatically faster. `pyarrow` reads the Parquet metadata, identifies which row groups *might* contain `category == 'A'`, and only loads those specific chunks from disk into memory. This is the single most important optimization for querying large Parquet datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cc1fc3-bc64-4b34-a9a9-c9803167ecc6",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ### 3. Handling Schema Evolution Gracefully\n",
    "\n",
    " Data schemas change. New columns are added, old ones are removed. A robust data format must handle this. Parquet, when read as a `dataset`, does this beautifully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d31810-dbb1-4312-a890-9802a485f307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Combined DataFrame with Evolved Schema ---\n",
      "   id   name\n",
      "0   1  Alice\n",
      "1   2    Bob\n",
      "2   3   None\n",
      "3   4   None\n",
      "\n",
      "Data types:\n",
      "id       int64\n",
      "name    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Version 1 of our data\n",
    "df_v1 = pd.DataFrame({'id': [1, 2], 'name': ['Alice', 'Bob']})\n",
    "pq.write_table(pa.Table.from_pandas(df_v1), f'{DATA_DIR}/schema_v1.parquet')\n",
    "\n",
    "# Version 2: we add an 'email' column and remove 'name'\n",
    "df_v2 = pd.DataFrame({'id': [3, 4], 'email': ['charlie@email.com', 'dave@email.com']})\n",
    "pq.write_table(pa.Table.from_pandas(df_v2), f'{DATA_DIR}/schema_v2.parquet')\n",
    "\n",
    "# Read the directory as a single dataset\n",
    "# We need to filter for just our schema files to avoid including others from this script\n",
    "schema_files = [f'{DATA_DIR}/schema_v1.parquet', f'{DATA_DIR}/schema_v2.parquet']\n",
    "dataset = pq.ParquetDataset(schema_files)\n",
    "combined_df = dataset.read().to_pandas()\n",
    "\n",
    "print(\"\\n--- Combined DataFrame with Evolved Schema ---\")\n",
    "print(combined_df)\n",
    "print(\"\\nData types:\")\n",
    "print(combined_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f302d338-d75c-463d-a93c-b816cf945947",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " **Explanation:** The `pyarrow.dataset` API reads all files, unifies their schemas, and fills missing values with `None` or `NaN`. This allows you to query a directory of evolving data without writing complex merging logic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2406b95-103d-4166-a749-7618bc621343",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ### 4. Natively Storing and Querying Nested Data\n",
    "\n",
    " Parquet isn't just for flat tables. It has first-class support for nested structures like lists and structs (dicts), which is incredibly useful for semi-structured data like event logs or API responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb530995-ba55-4b3b-977c-416af8ad981d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DataFrame with Nested Data ---\n",
      "   event_id                                   user_profile\n",
      "0       101  {'name': 'Eve', 'roles': ['admin', 'editor']}\n",
      "1       102         {'name': 'Frank', 'roles': ['viewer']}\n",
      "\n",
      "Accessing a nested element:\n",
      "{'name': 'Eve', 'roles': array(['admin', 'editor'], dtype=object)}\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Create a DataFrame with nested data\n",
    "nested_df = pd.DataFrame({\n",
    "    'event_id': [101, 102],\n",
    "    'user_profile': [\n",
    "        {'name': 'Eve', 'roles': ['admin', 'editor']},\n",
    "        {'name': 'Frank', 'roles': ['viewer']}\n",
    "    ]\n",
    "})\n",
    "\n",
    "file_path = f'{DATA_DIR}/nested_data.parquet'\n",
    "nested_df.to_parquet(file_path)\n",
    "\n",
    "# Read it back and see the structure is preserved\n",
    "read_nested_df = pd.read_parquet(file_path)\n",
    "\n",
    "print(\"\\n--- DataFrame with Nested Data ---\")\n",
    "print(read_nested_df)\n",
    "print(\"\\nAccessing a nested element:\")\n",
    "print(read_nested_df['user_profile'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91fcf23-bc24-466d-b5b6-3c1876ece071",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ### 5. Embedding Custom Metadata\n",
    "\n",
    " Want to embed the git hash, pipeline version, or data source in your file? Parquet's schema allows for custom key-value metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c61889-930d-4577-a90e-e9f9df66df8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# Create a PyArrow table, as pandas' to_parquet doesn't directly support this\n",
    "my_df = pd.DataFrame({'a': [1, 2, 3]})\n",
    "table = pa.Table.from_pandas(my_df)\n",
    "\n",
    "# Create custom metadata dictionary (values must be bytes for PyArrow)\n",
    "custom_meta = {\n",
    "    'pipeline_version': b'v1.2.3',\n",
    "    'source_system': b'production_db',\n",
    "    'git_commit_hash': b'a1b2c3d4e5f6'\n",
    "}\n",
    "\n",
    "# Add the metadata to the schema and write the file\n",
    "# updated_schema = table.schema.with_metadata(custom_meta)\n",
    "# table_with_meta = table.with_schema(updated_schema)\n",
    "# pq.write_table(table_with_meta, f'{DATA_DIR}/metadata.parquet')\n",
    "#\n",
    "# # Read the metadata back\n",
    "# read_schema = pq.read_schema(f'{DATA_DIR}/metadata.parquet')\n",
    "# retrieved_meta = {k.decode('utf-8'): v.decode('utf-8') for k, v in read_schema.metadata.items()}\n",
    "#\n",
    "# print(\"\\n--- Retrieved Custom Metadata ---\")\n",
    "# print(retrieved_meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f0e103-7f06-4727-aca8-be5ecfc2e6ff",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ### Conclusion\n",
    "\n",
    " We've gone far beyond basic reads and writes to unlock Parquet's true potential. By mastering **compression**, **predicate pushdown**, **schema evolution**, **nested types**, and **custom metadata**, you can build more efficient, scalable, and maintainable data systems.\n",
    "\n",
    " The next time you work with large-scale data, remember these tools. They are the difference between a slow, expensive pipeline and a fast, optimized one."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
