<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=description content="An example site for the clean and configurable Hugo Classless theme."><link href=/2025/05/sllm.nvim/ rel=alternate type=application/rss+xml title=mozanunal.com><link id=theme-style rel=stylesheet href=/css/classless.20c8e211f6768ba295effc3e101a31bfe248db7295bb122355bdf893c6cee731.css integrity="sha256-IMjiEfZ2i6KV7/w+EBoxv+JI23KVuxIjVb34k8bO5zE="><style>pre code,code{-webkit-text-size-adjust:100%;text-size-adjust:100%}img{display:block;margin-left:auto;margin-right:auto}figcaption{display:block;margin-left:auto;margin-right:auto}nav{display:flex;justify-content:space-between;align-items:center;flex-wrap:wrap;gap:1rem;overflow:visible;font-size:1.4rem}nav ul{display:flex;list-style:none;padding:0;margin:0;gap:.5rem}</style><link rel=stylesheet href=/css/syntax-light.min.d1e9974bf1fe0f3bfae6a7af3a04c1e749276b8efbbf6d3397998585f82443f0.css media="(prefers-color-scheme: light)"><link rel=stylesheet href=/css/syntax-dark.min.22964c63865be9217cedda7ac3dbd0143f8f41b3a0f681ef71e845b18a33a4f3.css media="(prefers-color-scheme: dark)"><title>Introducing sllm.nvim: A Neovim Plugin for Seamless LLM Integration | mozanunal.com</title><link rel=icon href=/favicon.svg type=image/svg+xml><link rel="shortcut icon" href=/favicon.ico><link rel=apple-touch-icon href=/apple-touch-icon.png></head><body><header><nav><ul><li><strong>mozanunal.com</strong></li></ul><ul><li><a href=/>Home</a></li><li><a href=/posts/>Posts</a></li><li><a href=/projects/>Projects</a></li><li><a href=/papers/>Papers</a></li></ul></nav></header><main><article><h1>Introducing sllm.nvim: A Neovim Plugin for Seamless LLM Integration</h1><p><em>May 21, 2025</em></p><div><p>As developers, we all know the pain of context-switching between our code editor
and separate AI interfaces just to ask a question, get code completion, or
analyze an error message. What if you could bring the power of Large Language
Models—powered by the incredibly flexible
<a href=https://llm.datasette.io/en/stable/><code>llm</code> CLI by Simon Willison</a>—<strong>directly
inside Neovim</strong>, with streaming replies, model/tool selection, and one-key
context management?</p><p>Today, I’m excited to announce
<a href=https://github.com/mozanunal/sllm.nvim><code>sllm.nvim</code></a>: a (roughly 500-line) Lua
plugin that brings deep <code>llm</code> CLI integration to Neovim in an asynchronous,
focused, and highly hackable way.</p><hr><h3 id=why-sllmnvim>Why sllm.nvim?</h3><p><a href=https://github.com/simonw/llm><code>llm</code></a> is a CLI Swiss Army knife for working
with LLMs, built by Simon Willison (creator of Django, Datasette, and
sqlite-utils). It supports <em>hundreds</em> of models (via plugins like
<a href=https://github.com/simonw/llm-openrouter>llm-openrouter</a>), files, tools, and
arbitrary context tricks.</p><p>But until now, integrating <code>llm</code> CLI into your real coding environment required
a lot of copy-paste—and breaking flow by toggling terminals, browsers, and
buffers.</p><p>I built <code>sllm.nvim</code> so you can:</p><ul><li><strong>Chat with models in a streaming popup buffer</strong> (no annoying freezes or
waiting)</li><li><strong>Add files, URLs, shell output, code selections, diagnostics—and even
tools!—to the LLM context… in one keystroke</strong></li><li><strong>Switch models and tools on the fly</strong></li><li><strong>Get token usage/cost feedback and never overrun your limits</strong></li></ul><p>…all <strong>without leaving Neovim</strong>. No more copy-paste error traces. No more
browser tab juggling. Just ask, code, and iterate, staying in the flow.</p><hr><h3 id=key-features>Key Features</h3><ul><li><strong>Interactive Chat:</strong> Ask questions and see responses stream directly into a
live, markdown-formatted Neovim buffer.</li><li><strong>Context Management:</strong> In one keystroke, you can add the current file, an
arbitrary URL, visual selections, diagnostics, shell command outputs, or LLM
tools to be considered in your next prompt.</li><li><strong>Model & Tool Selection:</strong> Instantly pick from any installed <code>llm</code>
model/plugins/tools and add them to context.</li><li><strong>Async, Non-blocking:</strong> Let LLM jobs run in the background—you keep editing
or exploring.</li><li><strong>Usage Feedback:</strong> Optionally show tokens/cost for every request.</li><li><strong>UI via mini.nvim:</strong> Uses proven modules from
<a href=https://github.com/echasnovski/mini.nvim>mini.nvim</a> for notifications and
pickers—smooth and minimal.</li></ul><h3 id=example-workflow>Example Workflow</h3><ul><li>Open a file, highlight some lines, hit <code>&lt;leader>sv</code> to add that code as
context.</li><li>Press <code>&lt;leader>ss</code> and enter your question.</li><li>Try <code>&lt;leader>sm</code> to switch models, e.g. between GPT-4 and local models.</li><li>Add a shell command’s output to context with <code>&lt;leader>sx</code>.</li><li>Add info on a bug by pressing <code>&lt;leader>sd</code> (diagnostics).</li><li>Need a third-party API or tool integration for your prompt? <code>&lt;leader>se</code> lets
you add LLM tools directly.</li><li>Review token/cost estimates with every response.</li></ul><p>All responses stream into a markdown buffer—ready to reference, copy, or save.</p><hr><h3 id=want-to-try-it>Want to Try It?</h3><p><strong>Check it out here:</strong>
<a href=https://github.com/mozanunal/sllm.nvim>https://github.com/mozanunal/sllm.nvim</a></p><p>I’d love to hear feedback, issues, and wishes. PRs and suggestions are very
welcome!</p><hr><h3 id=credits>Credits</h3><ul><li>LLM engine: <a href=https://github.com/simonw/llm>llm CLI by Simon Willison</a></li><li>UI/pickers:
<a href=https://github.com/echasnovski/mini.nvim>mini.nvim by echanovski</a></li><li>Plugin: <a href=https://github.com/mozanunal/sllm.nvim>mozanunal/sllm.nvim</a>
(MIT/Apache-2.0)</li></ul><p>Enjoy chatting and coding—without ever leaving Neovim!</p></div></article></main><footer><p>&copy; 2026 M.Ozan Unal.</p></footer></body><link rel=stylesheet href=/libs/katex/katex.min.v0.16.9.css><script defer src=/libs/katex/katex.min.v0.16.9.js></script><script defer src=/libs/katex/auto-render.v0.16.9.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1},{left:"$",right:"$",display:!1}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-007KSW65JL"></script><script>var doNotTrack=!1,dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes";if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-007KSW65JL")}</script></html>