<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><style>pre code,code{-webkit-text-size-adjust:100%;text-size-adjust:100%}</style><link id=theme-style rel=stylesheet href=https://cdn.jsdelivr.net/npm/@picocss/pico@2/css/pico.classless.min.css><title>Introducing sllm.nvim: A Neovim Plugin for Seamless LLM Integration | mozanunal.com</title><link rel=icon href=/favicon.svg type=image/svg+xml><link rel="shortcut icon" href=/favicon.ico><link rel=apple-touch-icon href=/apple-touch-icon.png></head><body><header><div class=theme-selector><select id=theme-selector-input><option value=https://cdn.jsdelivr.net/npm/@picocss/pico@2/css/pico.classless.min.css>Pico</option><option value=https://cdn.jsdelivr.net/gh/digitallytailored/classless@latest/classless.min.css>Classless.css</option><option value=https://cdn.jsdelivr.net/gh/cspablocortez/cosmocss@latest/cosmo.min.css>Cosmo.css</option><option value=https://cdn.jsdelivr.net/npm/@picocss/pico@2/css/pico.fluid.classless.min.css>Pico-Fluid</option><option value=https://cdn.jsdelivr.net/npm/water.css@2/out/water.css>Water.css</option><option value=https://cdn.jsdelivr.net/npm/sakura.css/css/sakura.css>Sakura.css</option><option value=https://cdn.simplecss.org/simple.min.css>Simple.css</option><option value=https://cdn.jsdelivr.net/gh/alvaromontoro/almond.css@latest/dist/almond.min.css>Almond.css</option><option value=https://cdn.jsdelivr.net/npm/@exampledev/new.css@1.1.2/new.min.css>New.css</option><option value=https://unpkg.com/missing.css>Missing.css</option><option value=https://unpkg.com/boltcss/bolt.min.css>Bolt.css</option><option value=https://perfectmotherfuckingwebsite.com/fuckingstyle.css>PerfectMFWS</option><option value>HTML only</option></select></div><script>(function(){"use strict";const o=document.getElementById("theme-style"),e=document.getElementById("theme-selector-input"),n="hugo-classless-theme";function s(t){o.href=t,e&&(e.value=t)}const t=localStorage.getItem(n);if(t){const n=Array.from(e.options).some(e=>e.value===t);n&&s(t)}e&&e.addEventListener("change",()=>{const t=e.value;s(t),localStorage.setItem(n,t)})})()</script><div><h3>mozanunal.com</h3><h4><a href=/>Home</a> | <a href=/posts/>Posts</a> | <a href=/projects/>Projects</a> | <a href=/papers/>Papers</a> | <a href=/contact/>Contact</a></h4></div></header><main><article><header><h1>Introducing sllm.nvim: A Neovim Plugin for Seamless LLM Integration</h1><p><em><time datetime=2025-05-21>May 21, 2025</time></em></p></header><div><p>As developers, we all know the pain of context-switching between our code editor
and separate AI interfaces just to ask a question, get code completion, or
analyze an error message. What if you could bring the power of Large Language
Models—powered by the incredibly flexible
<a href=https://llm.datasette.io/en/stable/><code>llm</code> CLI by Simon Willison</a>—<strong>directly
inside Neovim</strong>, with streaming replies, model/tool selection, and one-key
context management?</p><p>Today, I’m excited to announce
<a href=https://github.com/mozanunal/sllm.nvim><code>sllm.nvim</code></a>: a (roughly 500-line) Lua
plugin that brings deep <code>llm</code> CLI integration to Neovim in an asynchronous,
focused, and highly hackable way.</p><hr><h3 id=why-sllmnvim>Why sllm.nvim?</h3><p><a href=https://github.com/simonw/llm><code>llm</code></a> is a CLI Swiss Army knife for working
with LLMs, built by Simon Willison (creator of Django, Datasette, and
sqlite-utils). It supports <em>hundreds</em> of models (via plugins like
<a href=https://github.com/simonw/llm-openrouter>llm-openrouter</a>), files, tools, and
arbitrary context tricks.</p><p>But until now, integrating <code>llm</code> CLI into your real coding environment required
a lot of copy-paste—and breaking flow by toggling terminals, browsers, and
buffers.</p><p>I built <code>sllm.nvim</code> so you can:</p><ul><li><strong>Chat with models in a streaming popup buffer</strong> (no annoying freezes or
waiting)</li><li><strong>Add files, URLs, shell output, code selections, diagnostics—and even
tools!—to the LLM context… in one keystroke</strong></li><li><strong>Switch models and tools on the fly</strong></li><li><strong>Get token usage/cost feedback and never overrun your limits</strong></li></ul><p>…all <strong>without leaving Neovim</strong>. No more copy-paste error traces. No more
browser tab juggling. Just ask, code, and iterate, staying in the flow.</p><hr><h3 id=key-features>Key Features</h3><ul><li><strong>Interactive Chat:</strong> Ask questions and see responses stream directly into a
live, markdown-formatted Neovim buffer.</li><li><strong>Context Management:</strong> In one keystroke, you can add the current file, an
arbitrary URL, visual selections, diagnostics, shell command outputs, or LLM
tools to be considered in your next prompt.</li><li><strong>Model & Tool Selection:</strong> Instantly pick from any installed <code>llm</code>
model/plugins/tools and add them to context.</li><li><strong>Async, Non-blocking:</strong> Let LLM jobs run in the background—you keep editing
or exploring.</li><li><strong>Usage Feedback:</strong> Optionally show tokens/cost for every request.</li><li><strong>UI via mini.nvim:</strong> Uses proven modules from
<a href=https://github.com/echasnovski/mini.nvim>mini.nvim</a> for notifications and
pickers—smooth and minimal.</li></ul><h3 id=example-workflow>Example Workflow</h3><ul><li>Open a file, highlight some lines, hit <code>&lt;leader>sv</code> to add that code as
context.</li><li>Press <code>&lt;leader>ss</code> and enter your question.</li><li>Try <code>&lt;leader>sm</code> to switch models, e.g. between GPT-4 and local models.</li><li>Add a shell command’s output to context with <code>&lt;leader>sx</code>.</li><li>Add info on a bug by pressing <code>&lt;leader>sd</code> (diagnostics).</li><li>Need a third-party API or tool integration for your prompt? <code>&lt;leader>se</code> lets
you add LLM tools directly.</li><li>Review token/cost estimates with every response.</li></ul><p>All responses stream into a markdown buffer—ready to reference, copy, or save.</p><hr><h3 id=want-to-try-it>Want to Try It?</h3><p><strong>Check it out here:</strong>
<a href=https://github.com/mozanunal/sllm.nvim>https://github.com/mozanunal/sllm.nvim</a></p><p>I’d love to hear feedback, issues, and wishes. PRs and suggestions are very
welcome!</p><hr><h3 id=credits>Credits</h3><ul><li>LLM engine: <a href=https://github.com/simonw/llm>llm CLI by Simon Willison</a></li><li>UI/pickers:
<a href=https://github.com/echasnovski/mini.nvim>mini.nvim by echanovski</a></li><li>Plugin: <a href=https://github.com/mozanunal/sllm.nvim>mozanunal/sllm.nvim</a>
(MIT/Apache-2.0)</li></ul><p>Enjoy chatting and coding—without ever leaving Neovim!</p></div></article></main><footer><p>&copy; 2025 M.Ozan Unal.</p></footer></body><script async src="https://www.googletagmanager.com/gtag/js?id=G-007KSW65JL"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-007KSW65JL")}</script></html>